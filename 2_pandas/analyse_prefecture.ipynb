{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%HTML\n",
    "# ATELIER JOURNALISME DE DONNEES @OGP 2016\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "<img src='http://i0.wp.com/schoolofdata.org/files/2016/07/Data-pipeline-v2-EN.png?zoom=2&resize=207%2C444' width=150px style='float:right'></img>\n",
    "\n",
    "Ce document a été créé par [Cédric Lombion](https://twitter.com/clombion) à l'occasion d'un atelier d'1h30 pour journalistes lors de l'Open Government Partnership Summit de 2016, à Paris. L'objectif de cet atelier est d'offrir aux participants un aperçu d'une démarche simple mais professionelle de travail avec les données, à travers une introduction aux méthodologies, processus et outils suivants :\n",
    "\n",
    "* la [pipeline de données et l'expédition de données](http://schoolofdata.org/methodology/) de School of Data\n",
    "* la mise en place d'une démarche de préservation et de reproductibilité de son travail\n",
    "* le nettoyage et la vérification de données avec [Open Refine](http://openrefine.org), \n",
    "* la documentation et le développement de code avec [Jupyter Notebook](http://jupyter.org)\n",
    "* l'analyse et la visualisation de données avec [Pandas](http://pandas.pydata.org/)\n",
    "\n",
    "Le jeu de données utilisé, qui porte sur les délais d'attente pour une prise de rendez-vous en ligne à la préfecture, est par ailleurs au coeur d'une question d'actualité : l'accueil des étrangers en France. Ce jeu de données a été collecté par l'association française la [Cimade](www.lacimade.org) dans le cadre de son travail d'enquête et de lobbying [A guichets fermés](http://www.lacimade.org/publication/a-guichets-fermes/).\n",
    "\n",
    "Cet atelier s'inscrit dans le cadre du projet [Soutenir les solidarités](http://wiki.okfn.org/Projet_Soutenir_les_solidarites) de l'Ecole des données, un projet de Open Knowledge France, groupe local de School of Data, et présente une démarche inspirée (mais simplifiée) du travail [d'analyse en cours](https://github.com/clombion/analyse-attente-prefectures/blob/master/CIMADE/2.%20Pandas%20Analysis/CIMADE_cleaned_v1.ipynb) des données de La Cimade.\n",
    "\n",
    "-------\n",
    "\n",
    "Afin de minimiser le temps et les problèmes récurrents d'installation au début de l'atelier, les plateformes en ligne suivantes sont proposées comme alternatives :\n",
    "\n",
    "* [IBM Data Scientist Workbench](https://datascientistworkbench.com)(Free technology preview), pour utiliser OpenRefine en ligne.\n",
    "* [Domino DataLab](https://dominodatalab.com)(Free 14-day trial), pour utiliser Python et un Jupyter notebook en ligne.\n",
    "\n",
    "Dans l'éventualité d'un problème lié à internet ou au serveurs, des installateur pour [Anaconda](https://www.continuum.io/downloads) (suite logicielle qui contient Python, Pandas et Jupyter notebook entre autres) et [OpenRefine](https://github.com/OpenRefine/OpenRefine/wiki/Installation-Instructions) sont mis à diposition.\n",
    "\n",
    "------\n",
    "\n",
    "## Contexte\n",
    "\n",
    "La crise migratoire qui frappe la France et l'Europe depuis quelques années a connu une accélération suite à la guerre en Syrie et la montée en activité de l'Etat Islamique. Bien que la France ait été moindrement touchée par rapport à d'autres pays comme l'Italie (point d'atterissage des migrants passant par le Maghreb), la Turquie (zone de passage des migrants en route pour l'Europe de l'Ouest) et l'Allemagne (terre d'accueil sous l'impulsion de sa chancellière), l'absence d'une politique d'accueil cohérente et efficace a été à l'origine d'une gestion sur le terrain loin d^re à la hauteur. \n",
    "\n",
    "Fort heureusement, la France est doté d'un réseau d'associations professionelles et de collectifs citoyens dynamiques qui se sont vite mobilisés pour faire face à l'enjeu, et décrirer l'absence de réactivité de l'Etat Français, premier responsable de l'accueil des étrangers sur le sol français. Une de ces associations est La Cimade, qui se définit comme suit :\n",
    "\n",
    "> La Cimade a pour but de manifester une solidarité active avec les personnes opprimées et exploitées. Elle défend la dignité et les droits des personnes réfugiées et migrantes, quelles que soient leurs origines, leurs opinions politiques ou leurs convictions.\n",
    "\n",
    "<img src='http://aguichetsfermes.lacimade.org/images/Prefecture_Couv200.jpg'  style='float:right; margin:20px'></img>\n",
    "\n",
    "Dans la cadre d'une campagne pour pousser le gouvernement français à améliorer sa politique d'accueil, La Cimade a créé le projet _[A guichets fermés](http://www.lacimade.org/publication/a-guichets-fermes/)_ qui vise à dénoncer les pratiques condamnables voir illégales des préfectures françaises conduisant, intentionellement ou non, à léser les migrants et résidents étrangers dans leurs droits. En effet, si l'Etat français est directement responssable de la politique d'accueil, ce sont les préfectures, instutitutions représentatives de l'autorité de l'Etat sur le territoire, qui sont chargées d'accueillir au quotidien les résidents étrangers dans leurs démarches.\n",
    "\n",
    "Or, comme l'a montré la campagne de La Cimade, de nombreux obstacles sont mis sur le chemin des étrangers voulant réaliser une démarche administrative en préfecture, en commençant par la prise de rendez-vous. Depuis 2012, les préfectures dématérialisent de plus en plus la démarche de prise de rendez-vous, ce qui posent des problèmes pour de nombreuses étrangers non connectés ou peu familiers avec les démarches en ligne. S'ajoutent fréquemment à cela une impossibilité effective de prendre rendez-vous par un autre biais (alors que la loi l'oblige), et des délais extravagants d'attente en passant par internet.\n",
    "\n",
    "Afin de démontrer l'existence de ce problème, La Cimade procède depuis 2015 à une collecte de données automatisées sur les sites des préfectures, afin d'évaluer la possibilité de prendre un rendez-vous et si oui, le temps d'attente. La démarche est décrite [ci-après](http://aguichetsfermes.lacimade.org/) :\n",
    "\n",
    "> Un robot va visiter le site de la préfecture toutes les heures : il regarde si des rendez-vous sont disponibles et note, le cas échéant, les 2 premières dates disponibles. Il réalise au passage les \"captures d'écran\" des pages consultées permettant, par la suite, de vérifier ses résultats.\n",
    "\n",
    "> Le robot ne prend jamais de rendez-vous.\n",
    "\n",
    "Au-delà de l'ingénuité de la démarche, la démarche est louable pour sa transparence : le code source du bot et les résultats sont disponibles, dans des formats réutilisables. Cette démarche d'ouverture des données est à encourager, car de nombreuses associations détiennent des données d'intérêt général qui restent encore inaccessibles aux journalistes, chercheurs voire aux autres association par défaut d'ouverture.\n",
    "\n",
    "Et bien que La Cimade a déjà exploité les données en publiant un rapport et une visualisation interactive, nous allons nous plonger dedans afin de voir si d'autres questions informations ou questions intéressantes émergent.\n",
    "\n",
    "## La pipeline des données\n",
    "\n",
    "Cet atelier fait régulièrement référence à différentes étapes d'une méthodologie formalisée par School of Data en 2012 appelée la pipeline des données (data pipeline). Elle regroupe différentes étapes considérées essentielles à tout travail avec les données :\n",
    "\n",
    "* **Définir** son sujet, sa question, ou le problème que l'on essaie de résoudre\n",
    "* **Trouver** les données qui permettront d'avoir des réponses\n",
    "* **Récupérer** les données où qu'elles se trouvent : serveur, site web, pdf, papier...\n",
    "* **Vérifier** les données que l'on a récupérées afin de valider leur pertinence et/ou leur qualité\n",
    "* **Nettoyer** les données afin de les rendre lisibles (par l'ordinateur) ou plus simples à analyser\n",
    "* **Analyser** pour en tirer des réponses ou de nouvelles questions\n",
    "* **Présenter** les données pour un public ou pour identifier visuellement des tendances\n",
    "\n",
    "La pipeline des données est l'élément structurant du format d'atelier appelé _Expédition de données_ dont cet atelier est une version allégée.\n",
    "\n",
    "## Définir\n",
    "\n",
    "Le problème de départ ici est simple : il s'agit d'explorer les données de la campagne _A guichets fermés_ de La Cimade afin de comprendre plus en détail le phénomène qu'elles décrivent. En effet, bien que La Cimade utilise une visualisation astucieuse pour montrer très simplement le niveau de fiabilité des différents systèmes de réservations des préfectures, elle perd en précision et capacité de comparaison ce qu'elle gagne en vitesse de lecture :\n",
    "\n",
    "<img src='https://dl.dropboxusercontent.com/u/27908799/OGP_WORKSHOP/RESOURCES/cimade_viz.png'></img>\n",
    "\n",
    "Un des problèmes saillants que La Cimade met en évidence est celui du délai d'attente avant un rendez-vous. Il s'agira donc pour nous de d'**explorer les données pour se faire une idée du délai d'attente réel et des disparités entre préfectures.**\n",
    "\n",
    "> **Note**\n",
    "\n",
    "> Le fichier mis à disposition par La Cimade fait près de 600 000 lignes. En plus d'être difficile à utiliser dans les logiciels de feuille de calcul classiques (Excel, LibreOffice Calc), il peut facilement mettre en difficultés des ordinateurs avec de faibles capacités. Le fichier de travail ici a donc été réduit aux seules préfectures de la petite couronne parisienne, ce qui le rend 10 fois plus léger que le fichier original.\n",
    "  \n",
    "## Trouver\n",
    "\n",
    "Nous allons travailler sur un extrait du fichier de la Cimade ([disponible ici](https://dl.dropboxusercontent.com/u/27908799/OGP_WORKSHOP/0.%20ORIGINAL/OGP_CIMADE.csv)), qui ne concerne que les départements de la petite couronne parisienne : Hauts de Seine (92), Seine-saint-Denis (93), Val de Marne (94).\n",
    "\n",
    "> **Note**\n",
    "\n",
    "> Le fichier original de La Cimade est disponible [ici](http://aguichetsfermes.lacimade.org/csv.php).\n",
    "\n",
    "## Récupérer\n",
    "\n",
    "La Cimade ayant publié le fichier au format CSV et accessible d'un clic, cette étape est une formalité. C'est cependant une étape cruciale du processus : Une fois téléchargé, où mettre le fichier ? L'enjeu ici est de s'assurer de ne pas endommager ou modifier le fichier original pour être certain de pouvoir y revenir. Une façon simple de procéder peut être la suivante :\n",
    "\n",
    "1. Créez un dossier (dans votre dossier `Documents` ou autre dossier où vous mettez vos projets) appelé `0_original` où vous déposez le fichier csv. \n",
    "2. **Copiez-le fichier original immédiatement**. \n",
    "3. Dans le même dossier où se trouve `0_original`, créez `1_openrefine`, et glissez-y la copie.\n",
    "3. Sachant que vous nous allons travailler avec Pandas, on peut dès maintenant créer un dossier `2_^pandas`.\n",
    "\n",
    "Cela devrait ressembler à ça:\n",
    "\n",
    "<img src='https://dl.dropboxusercontent.com/u/27908799/OGP_WORKSHOP/RESOURCES/recuperer1.png' width=400px></img>\n",
    "\n",
    "Si vous devez travailler avec plusieurs datasets, vous pouvez soit reproduire cet arrangement pour les autres datasets ou créer des dossiers individuels au sein de chaque étape (si les étapes du process ne diffèrent pas trop). Vous pouez aussi créer un dossier 'Resources' pour y mettre des ressources contextuelles dont vous avez besoin pour votre analyse (rapports pdf, images...)\n",
    "\n",
    "\n",
    "## Nettoyer et Vérifier\n",
    "\n",
    "Bien que la pipeline des données semble être linéaire, ce n'est souvent pas le cas dans la réalité  du travail avec les données, comme on va le voir une fois le fichier chargé dans OpenRefine.\n",
    "\n",
    "> Note\n",
    "\n",
    "> On va travailler ici avec l'outil de travail avec les données en lignes de IBM, le Datascience Workbench. L'inscription est totalement gratuite - ne vous sentez pas obligés de mettre vos vraies données dans le formulaire (à l'exception de l'adresse, qu'il faut pouvoir accéder pour valider l'inscription).\n",
    "\n",
    "### Chargement du fichier dans OpenRefine\n",
    "\n",
    "1. On crée un compte sur le site du [IBM Datascience Workbench](https://datascientistworkbench.com). Si vous choisissez de télécharger et d'installer OpenRefine, ça se passe [par ici](http://openrefine.org/download.html). Une fois installé, vous pouvez passer directement à l'étape 4.\n",
    "     <img src='https://dl.dropboxusercontent.com/u/27908799/OGP_WORKSHOP/RESOURCES/define1.png' width=400px></img>\n",
    "2. On choisit ensuite l'application OpenRefine\n",
    "     <img src='https://dl.dropboxusercontent.com/u/27908799/OGP_WORKSHOP/RESOURCES/define2.png' width=400px></img>\n",
    "3. OpenRefine va prendre un peu de temps à charger, le temps que le serveur démarre\n",
    "     <img src='https://dl.dropboxusercontent.com/u/27908799/OGP_WORKSHOP/RESOURCES/define3.png' width=400px></img>\n",
    "4. Nous voilà à l'accueil du logiciel. L'interface est exactement la même que la version hors ligne d'OpenRefine. Et selon le même procédé, on va cliquer sur `Parcourir` pour sélectionner notre fichier `OGP_CIMADE.csv`, puis `Next`pour le charger dans OpenRefine.\n",
    "5. Dernière étape avant de pouvoir travailler avec le fichier : l'interface de paramétrage de l'import. Deux choses à faire ici : S'assurer que l'encodage UTF-8 est bien sélectonné (il faut cliquer dans le rectangle blanc pour faire apparaître la fenêtre de sélection) et donner un nom clair au projet (en haut à gauche).\n",
    "     <img src='https://dl.dropboxusercontent.com/u/27908799/OGP_WORKSHOP/RESOURCES/nettoyer_verifier5.png' width=400px></img>\n",
    "\n",
    "### Nettoyage du fichier\n",
    "\n",
    "1. Avant de pouvoir vérifier que le fichier ne contienne aucune valeur anormale, il va falloir le nettoyer. Un premier geste à faire est d'enlever, pour toutes les colonnes, les potentiels espaces avant et après le texte. Pour cela, il faut cliquer sur la flèche dans le coin haut droit d'une colonne et sélectionner : `Edit cells --> Common Transforms --> Trim leading and trailing whitespace`\n",
    "     <img src='https://dl.dropboxusercontent.com/u/27908799/OGP_WORKSHOP/RESOURCES/nettoyer_verifier6.png' width=400px></img>\n",
    "2. La seconde étape est de séparer le nom de la préfecture de celui de la procédure. Dans le menu de la colonne `Nom de la procédure` il faut aller à `Edit columns --> Split into several columns`.\n",
    "3. Dans la fenêtre qui s'ouvre, il faut choisir le séparateur : celui qui fait sens ici est ` : `, c'est à dire _espace-deux points-espace_.\n",
    "     <img src='https://dl.dropboxusercontent.com/u/27908799/OGP_WORKSHOP/RESOURCES/nettoyer_verifier7.png' width=400px></img>\n",
    "4. L'étape suivante est légèrement plus technique. Il va falloir fusionner les colonnes `Date du sondage` et `Heure du sondage`. Pourquoi ? Parce qu'on veut qu'Open Refine reconnaisse que ce sont des données de temps, hors l'ordinateur ne calcule jamais les dates et heures séparément. Même si une date affiche `31-12-2016`, l'ordinateur comprend `31-12-2016 00:00:00`. Cela marche pareil pour les heures seules, sauf que cette fois-ci l'ordinateur va rajouter une date par défaut, qui sera évidemment fausse. Par ailleurs, il n'y a aucun besoin de séparer les dates et les heures pour faire des calculs de temps, même sur les heures. Il nous faut donc joindre ces deux colonnes. Pour ce faire, on va trouver dans le menu de la colonne `Date du sondage` l'option `Edit Column --> Add Column based on this column...`.\n",
    "5. Dans le fenêtre qui s'ouvre, il va falloir écrire le nom de la nouvelle colonne (j'ai choisi `date_scraping`) et écrire la formule qui va permettre de créer une colonne qui additionne les valeurs des deux colonnes. La formule est la suvante: `cells['Date du sondage'].value + \"T\" + cells['Heure du sondage'].value`. Elle se lit comme ceci : le contenu (value) des champs (cells) de la colonne (symbolisée par les crochets) 'Date du sondage' auquel on ajoute T, auquel on ajoute le contenu des champs de la colonne 'Heure du sondage'. Pourquoi T ? C'est simplement la convention utilisée par OpenRefine pour symboliser l'heure. Cela facilite l'étape suivante.\n",
    "     <img src='https://dl.dropboxusercontent.com/u/27908799/OGP_WORKSHOP/RESOURCES/nettoyer_verifier8.png' width=400px></img>\n",
    "6. On peut maintenant faire comprendre à OpenRefine que notre nouvelle colonne `date_scraping` contient des données temporelles. Dans le menu de la colonne, sélectionnez `Edit cells --> Common Transform --> To date`. Le texte de la colonne devrait devenir vert, signe que OpenRefine reconnaît que c'est un type de données particulier.\n",
    "     <img src='https://dl.dropboxusercontent.com/u/27908799/OGP_WORKSHOP/RESOURCES/nettoyer_verifier9.png' width=400px></img>\n",
    "7. Tant que l'on est dans le nettoyage, finissons le travail. Il faut:\n",
    "     * Supprimer les colonnes `Heure du sondage` et `Date du sondage`\n",
    "     * renommer les colonnes restantes pour garder des uniquement des noms en minuscules, sans espaces, et simples à comprendre. J'ai choisi: `departement`, `prefecture`, `procedure`, `date_scraping`, `proposition1`, `proposition2`.\n",
    "\n",
    "### Première vérification des données\n",
    "\n",
    "Le premier enjeu de la vérfication des données est de s'assurer que le jeu de données est pertinent par rapport au problème que l'on a défini. Pas de souci ici, les données sont notre de point de départ initial, et elles sont très détaillées. Le second enjeu est l'intégrité des données. Est-ce qu'il y a des erreurs, des valeurs manquantes là où il devrait y en avoir, des valeurs aberrantes ? Une autre question que l'on pourrait poser ici est : est-ce que l'instrument de mesure (le bot) n'a pas eu de problème lors de la collecte de données ?\n",
    "\n",
    "Le seul moyen de répondre avec une confiance absolue cette dernière question est d'inspecter le code source. Sauf que si on est pas un développeur, ce n'est pas une option. Mais on peut observer la fiabilité du bot indirectement : puisque sont seul travail est de collecter les données à intervallles régulières, on peut regarder l'activité au cours du temps afin de s'assurer qu'il n'y a pas de comportement étrange. \n",
    "\n",
    "Pour observer cette activité, on va utiliser un autre outil de OpenRefine: le facet. Il permet de filtrer les données de diverses façons, s'avérant très pratiques pour de mutiples usages. Ici, on va utiliser le facet timeline (`menu --> facet --> Timeline facet`) sur la colonne `date_scraping` afin de vérifier que l'activité du bot est restée constante au cours du temps.\n",
    "\n",
    "<img src='https://dl.dropboxusercontent.com/u/27908799/OGP_WORKSHOP/RESOURCES/nettoyer_verifier10.png' width=400px></img>\n",
    "\n",
    "L'activité est plutôt constante d'un bout à l'autre de la ligne de temps, mais on observe quelques petites baisses ici et là, en plus d'une forte chute à la fin. Il sera donc nécessaire de demander des explications au développeur de La Cimade. On observe par ailleurs, en bougeant le curseur, que la fichier s'arrête en mai 2016. Or, le site web laisse entendre que le projet est toujours actif. C'est encore un autre point à vérifier avec La Cimade.\n",
    "\n",
    "On peut ensuite appliquer un facet à la colonne `procedure` (`menu --> facet --> Text facet`) pour identifier toutes les types de procédures et vérifier que certaines ne diffèrent pas simplement à cause d'une faute de frappe. \n",
    "\n",
    "<img src='https://dl.dropboxusercontent.com/u/27908799/OGP_WORKSHOP/RESOURCES/nettoyer_verifier11.png' width=400px></img>\n",
    "\n",
    "Il n'y a priori pas de faute de frappe, mais le facet permet d'observer une tendance intéressante : bien que le bot de la Cimade semble scrapper toutes les procédures disponibles en ligne, comme le montre la présence des procédures \"Permis de conduire\" et Plaques automobiles\", la grande majorité des procédures sont destinés à un public de résidents étrangers. C'est une tendance étrange qu'il faudra confirmer avec La Cimade et avec les préfectures concernées.\n",
    "\n",
    "Enfin, finissons avec un double facet. Un facet de text sur la colonne `département` et un autre sur la colonne `prefecture`. Il s'agit ici d'utiliser le fait que les facets s'affectent entre eux. En effet, cliquer sur un élément du facet revient à filtrer les données par cet élément (il est aussi possible d'exclure juste cet élément). Je peux donc très rapidement voir que les noms de départements sont associés aux bons numéros, et qu'aucun département ne se retrouve associé à plusieurs numéros.\n",
    "\n",
    "<img src='https://dl.dropboxusercontent.com/u/27908799/OGP_WORKSHOP/RESOURCES/nettoyer_verifier12.png' width=250px></img>\n",
    "\n",
    "Il ne reste aucune manipulation à faire dans OpenRefine : le fichier est nettoyé, vérifée, il ne reste plus qu'à l'expporter. Cela se fait dans le coin haut droit de l'interface, via `Export --> Comma-separated value` pour exporter le fichier csv nettoyé et `Export --> Export project` pour exporter le dossier de projet d'OpenRefine, qui conserve tout l'histoirique de modification de votre travail. Mettez ces deux fichiers dans le dossier `1_openrefine` et créer une copie du nouveau fichier csv pour le placer dans `2_pandas`.\n",
    "\n",
    "## Vérifier et Analyser\n",
    "\n",
    "### Ouverture du notebook\n",
    "\n",
    "Nous changeons de programme pour passer à Jupyter notebook et Pandas, mais une chose ne change pas : on ne fait pas confiance aux données. Même pendant l'analyse, il s'agira de rester alerte pour s'assurer qu'il n'y a rien de bizarre dans les données, ce qui pourrait fausser notre analyse.\n",
    "\n",
    "Pour travailler avec Jupyter notebook et Pandas, il est possible de travailler hors ligne en installant les bonnes librairies Python, mais nous allons nous faciliter la tâche en utilisant une platforme en ligne, [Domino DataLab](https://dominodatalab.com). Ils ont une version d'essai de 14 jours, parfaite pour ceux qui n'ont pas encore tout installé sur l'ordi.\n",
    "\n",
    "<img src='https://dl.dropboxusercontent.com/u/27908799/OGP_WORKSHOP/RESOURCES/verifier_analyser1.png' width=400px></img>\n",
    "\n",
    "Une fois dans l'interface il faut cliquer sur le bouton `New project`, lui donner un nom et valider. On se retrouve directement dans l'interface de mise en ligne des fichiers. Il suffit alors d'y glisser le csv créé via OpenRefine et attendre la fin de l'upload. Une fois le fichier en ligne, il faut cliquer sur le bouton `Run`, dans la barre latérale.\n",
    "\n",
    "<img src='https://dl.dropboxusercontent.com/u/27908799/OGP_WORKSHOP/RESOURCES/verifier_analyser2.png' width=400px></img>\n",
    "\n",
    "Dans l'interface qui s'affiche, il faut cliquer sur le bouton `notebook`, attendre que la session se charge puis cliquer que `Open session`. Et voilà! \n",
    "\n",
    "> **Note**\n",
    "\n",
    "> Chaque session est limitée à 1h, mais il suffit de la relancer après 1h pour continuer à travailler sans problème. Le seul bémol est que les librairies du serveur ne sont pas complètement à jour, ce qui fait des bugs déjà corrigés dans la version la plus récente apparaissent parfois.\n",
    "\n",
    "Nous sommes maintenant dans l'interface classique de Jupyter notebook. Si tout va bien, le fichier que l'on a mis en ligne est visible ici. Cependant on ne va pas y toucher, c'est depuis le notebook que l'on va le charger. Il faut ici cliquer en haut à droite pour créer un nouveau notebook : `New --> Python2`.\n",
    "\n",
    "### Préparation à l'analyse\n",
    "\n",
    "Commençons par importer les librairies qui nous intéressent et notre fichier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#import de la librairie d'analyse\n",
    "import pandas as pd\n",
    "\n",
    "#import de la librairie de visualisations\n",
    "import seaborn as sns\n",
    "\n",
    "#permet de voir les graphique directement dans le notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# on importe notre fichier csv créé avec OpenRefine. \n",
    "# Le encoding='utf-8' va nous éviter des problèmes courants avec Python2\n",
    "df = pd.read_csv('OGP_CIMADE_OPENREFINE.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a fait deux choses importantes ici :\n",
    "* on a importé pandas, et toutes ses fonctions. Cependant pour éviter de confondre des associées à différentes librairies mais dont le nom est similaire, on donne un acronyme à panda (pd), qu'il faudra accoler à toutes les fonctions qu'on utilise et qui viennent de cete librairie\n",
    "* on a importé le contenu de notre csv et l'avons créé un dataframe (df) à partir de ce contenu. Un _Dataframe_ est simplement la formalisation que pandas utilise pour parler de tableau de données. Les colonnes sont elles appelées _Series_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# on vérifie la tête de notre fichier\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clarifions les questions auxquelles ont veut une réponse. Maintenant que l'on a une bonne idée de ce que contient le jeu de données, on peut être plus précis.\n",
    "\n",
    "* Les questions que l'on a déjà\n",
    "     1. Pourquoi procédures en ligne concernent-elles majoritairement les usagers étrangers ?\n",
    "     2. Pourquoi le scrapper de la CIMADE a des variations d'activité ?\n",
    "     3. Pourquoi le fichier s'arrête-t-il en mai 2016 ?\n",
    "\n",
    "\n",
    "* Les questions auxquelles on veut répondre\n",
    "     1. Quel est le délai maximum auquel font face le usagers ?\n",
    "     2. Quel est le temps d'attente moyen par sous-préfecture pour les usagers ?\n",
    "     2. Quelles sont les meilleures et pire sous-préfectures pour les usagers ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation des données\n",
    "\n",
    "Répondre à nos questions demande une petite préparation des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# on regarde les types de données\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas reconnaît bien que les numéros de département sont des chiffres entiers (int64), mais il considère toutes les autres colonnes comme du text (object). Cependant on veut faire des opérations sur les colonnes de temps, pour calculer le délai par exemple. Il nous faut donc utiliser une fonction de Pandas permettant la conversion :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# on tranforme les trois colonnes avec des données temporelles en colonnes dont le type est \"datetime\".\n",
    "df['date_scraping'] = pd.to_datetime(df['date_scraping'])\n",
    "df['proposition1'] = pd.to_datetime(df['proposition1'])\n",
    "df['proposition2'] = pd.to_datetime(df['proposition2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# on vérifie que la conversion a bien fonctionné.\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# on crée une nouvelle colonne qui contiendra le délai d'attente : date du scraping moins première proposition de rdv\n",
    "df['delai'] = df['proposition1'] - df['date_scraping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# on vérifie que la colonne a bien été créée\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pour contourner certaines difficultés que Pandas a avec les dates, on crée une nouvelle colonne qui correspond aux\n",
    "# valeurs de la colonne délai, mais converties en secondes.\n",
    "df['delai_int'] = df['delai'].dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# on vérifie que la nouvelle colonne a bien été créée.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# les secondes ne sont pas très parlantes pour nous. \n",
    "# On divise les valeurs de la colonne delai_int par 60 (pour avoir des minutes), 60 (pour avoir des heures)\n",
    "# et 24 (pour avoir des jours)\n",
    "df['delai_int'] = df['delai_int'] / (60 * 60 * 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# On vérifie que l'on a bien les valeurs qu'on attend\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 : Délai maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# on calcule le délai maximum que les usagers doivent attendre.\n",
    "df['delai'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "94 jours correspond à plus de trois mois d'attente. Est-ce un temps d'attente courant ou une valeur exceptionnelle ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# on regarde les propriétés statistiques du dataframe\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La moyenne étant à 6 jours, on penche plutôt pour une valueur exceptionelle. Mais est-ce qu'on parle d'une seule préfecture avec une valeur exceptionnelle ou plusieurs ? Vérifions en regardant les valeurs maximum pour chaque préfecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# on groupe le tableau par préfecture, et on affiche la valeur max correspondante dans les autres colonnes.\n",
    "df.groupby('prefecture').max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le problème se trouve donc au niveau de la Sous-préfecture de Nogent-sur-Marne, qui dépasse les trois mois alors que les autres ont un maximum d'attente aux environs d'un mois. Cela dit la Sous-préfecture d'Antony n'affiche aucune valeur. Il semble que les procédures en ligne ne fonctionne jamais. Est-ce un problème informatique ? Il faudra poser la question. Recentrons-nous sur la préfecture de Nogent-sur-Marne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# jetons un oeil à toutes les fois où le scraper a vu une attente de plus de 3 mois\n",
    "df[df['delai_int'] > 90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les délais extrêmes observés à Nogent-sur-Marne sont tous sur un seul jour, ce qui peut être expliqué soit par un problème informatique ou par un lien avec les vacances de fin d'année. \n",
    "\n",
    "On peut tenter de vérifier visuellement l'hypothèse des vacances de fin d'année en regardant comment évoluent les temps d'attente moyens par semaine pour toutes les préfectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 : Le temps d'attente moyen par préfecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# lorsqu'on travaille avec des données temporelles, il devient vite nécessaire d'assigner la colonne avec les données\n",
    "# temporelles de référence à l'index, de façon à profiter des fonctions qui font des calcults de temps via l'index\n",
    "df.index = df['date_scraping']\n",
    "\n",
    "# On utilise ici la fonction pivot_table de Pandas, qui fonctionne comme une feuille de calcul,\n",
    "# avec plus de fonctionnalités. \n",
    "# Ici on groupe l'index par semaine, et on regarde les moyennes de delai pour chaque préfecture\n",
    "df_grouped = df.pivot_table(index=pd.TimeGrouper('1W'), values='delai_int', columns='prefecture', aggfunc='mean')\n",
    "\n",
    "# on vérifie le nouveau dataframe, df_grouped\n",
    "df_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# on peut maintenant visualiser la moyenne des délais par semaines pour chaque préfecture,\n",
    "# ce qui nous permet d'analyser les tendances et de savoir où diriger la suite de notre analyse\n",
    "df_grouped.plot(figsize=(20,40), subplots=True, sharey=True, ylim=(0,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Il y effectivement une augmentation visuelle du temps d'attente moyen aux alentours de Janvier. Mais ce qui saute aux yeux ici est un point que l'on a observé mais pas creusé : l'absence de possibilité de prendre un rendez-vous. Et au vu des profils de certaines sous-préfectures, la vraie question est de savoir pourquoi de tels dysfonctionnements semblent apparaître. Et là c'est un travail de journalisme classique qui commence, appuyé par les données."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
